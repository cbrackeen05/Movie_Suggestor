{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Everyday Problems: What do I watch next?\n",
    "\n",
    "In this project, I aim to solve the age-old problem of what to watch next. People know what movies they like, or even love, but how many duds does one have to go through to find the next great movie they will enjoy? I aim to significantly reduce the amount of time a person would need both to decide on a movie to watch as well as find a movie they will enjoy. I hope to build a program that takes in information about the current user that is looking to see a movie in the form of three movies and plots that the user already knows they enjoy and recommends the next movie that they should not only watch but should enjoy based on their preferences. With people wanting to move quicker and integrate more automation into their lives, this could be a huge success. Additionally, if the theory behind the program works, this could be extrapolated to other sectors such as recommending restaurants, nightlife, books, or anything else!\n",
    "\n",
    "The data in this project is scraped from two sources. Our list of popular titles in a genre comes from Ranker, a site that allows the public to vote on which movies they feel are the best of all time in a particular category. Our movie plot, rating, and year data comes from IMDB via the OMDB Api, a site that is a large database of all movie data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.decomposition import NMF\n",
    "import random\n",
    "import os\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing Title, Year, Rating, Plot data scraped from IMDB\n",
    "comedy = pd.read_json('CMovies.json', orient='records')\n",
    "horror = pd.read_json('HorMovies.json', orient='records')\n",
    "action = pd.read_json('ActMovies.json', orient='records')\n",
    "crime = pd.read_json('CriMovies.json', orient='records')\n",
    "drama = pd.read_json('DraMovies.json', orient='records')\n",
    "family = pd.read_json('FamMovies.json', orient='records')\n",
    "history = pd.read_json('HisMovies.json', orient='records')\n",
    "romance = pd.read_json('RomMovies.json', orient='records')\n",
    "scifi = pd.read_json('SFMovies.json', orient='records')\n",
    "thriller = pd.read_json('ThrMovies.json', orient='records')\n",
    "western = pd.read_json('WesMovies.json', orient='records')\n",
    "\n",
    "# Importing movie titles data scraped from Ranker\n",
    "comedy_title = pd.read_json('Comedy.json', orient='records')\n",
    "horror_title = pd.read_json('Horror.json', orient='records')\n",
    "action_title = pd.read_json('Action.json', orient='records')\n",
    "crime_title = pd.read_json('Crime.json', orient='records')\n",
    "drama_title = pd.read_json('Drama.json', orient='records')\n",
    "family_title = pd.read_json('Family.json', orient='records')\n",
    "history_title = pd.read_json('History.json', orient='records')\n",
    "romance_title = pd.read_json('Romance.json', orient='records')\n",
    "scifi_title = pd.read_json('SciFi.json', orient='records')\n",
    "thriller_title = pd.read_json('Thriller.json', orient='records')\n",
    "western_title = pd.read_json('Western.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data to replace any characters we don't want to applies to every dataset in lst (list)\n",
    "# Input for Clean data is a list of genre data\n",
    "def clean_data(lst):\n",
    "    for item in lst:\n",
    "        # Clean up year information\n",
    "        item.Year = item.Year.apply(lambda x: str(x).replace('â€“','') )\n",
    "        item.Year = item.Year.apply(lambda x: int(x))\n",
    "        \n",
    "        #Get rid of N/A in Plot and rating\n",
    "        item.Plot = item.Plot.apply(lambda x: str(x).replace('N/A', ''))\n",
    "        \n",
    "        #Setting N/A to 0 for rating as ratings are between 1-10\n",
    "        item.Rating = item.Rating.apply(lambda x: str(x).replace('N/A', '0'))\n",
    "        item.Rating = item.Rating.apply(lambda x: float(x))\n",
    "        \n",
    "    return lst\n",
    "\n",
    "# Collecting all genre data into one list\n",
    "genre_data = [comedy,horror,action,crime,drama,family,history,romance,scifi,thriller,western]\n",
    "\n",
    "# Applying data cleaning fucntion to our list of genre data\n",
    "genre_data = clean_data(genre_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We will use methods to analyze each set of plots to identify common digrams of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_digrams(genre):\n",
    "    # Combine all plots into one string\n",
    "    Genre_Plots = []\n",
    "    for plot in genre.Plot:\n",
    "        Genre_Plots.append(plot)\n",
    "    Genre_Plots = ' '.join(Genre_Plots)\n",
    "\n",
    "    # We want to use the standard english-language parser.\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    # Parsing genre plots\n",
    "    genre_plots = parser(Genre_Plots)\n",
    "\n",
    "    # Create list of all two-word phrases\n",
    "    word = [word for word in genre_plots if((word.is_stop==False) and (word.is_punct==False) and (word.pos_=='NOUN' or word.pos_=='ADJ'))]\n",
    "    genre_pair = []\n",
    "    for i,item in enumerate(word):\n",
    "        if i+1 < len(word):\n",
    "            genre_pair.append(str(item)+' '+str(word[i+1]))\n",
    "\n",
    "    pairs = set(genre_pair)\n",
    "\n",
    "    # Creating a grid indicating whether words are within 4 places of the target word\n",
    "    adjacency=pd.DataFrame(columns=pairs,index=pairs,data=0)\n",
    "\n",
    "\n",
    "    # Iterating through each word in the text and indicating which of the unique words are its neighbors\n",
    "    for i,word in enumerate(genre_pair):\n",
    "        #Making sure to stop at the end of the string, even if there are less than four words left after the target.\n",
    "        end=max(0,len(genre_pair)-(len(genre_pair)-(i+15)))\n",
    "        # The potential neighbors.\n",
    "        nextwords=genre_pair[i+1:end]\n",
    "        # Filtering the neighbors to select only those in the word list\n",
    "        neighbors=[nextwords[i] for i in range(len(nextwords))]\n",
    "        adjacency.loc[word,neighbors]=adjacency.loc[word,neighbors]+1\n",
    "\n",
    "    # Running TextRank\n",
    "    nx_pairs = nx.from_numpy_matrix(adjacency.as_matrix())\n",
    "    ranks=nx.pagerank(nx_pairs, alpha=.85, tol=.00000001)\n",
    "\n",
    "    # Identifying the five most highly ranked keywords\n",
    "    ranked = sorted(((ranks[i],s) for i,s in enumerate(pairs)),\n",
    "                    reverse=True)\n",
    "    \n",
    "    # Get only the digrams and store them to be returned\n",
    "    top_five = []\n",
    "    for dia in ranked[:5]:\n",
    "        top_five.append(dia[1])\n",
    "    \n",
    "    return top_five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_dict = {\n",
    "    'comedy': top_digrams(comedy),\n",
    "    'horror': top_digrams(horror),\n",
    "    'family': top_digrams(family),\n",
    "    'action': top_digrams(action),\n",
    "    'drama': top_digrams(drama),\n",
    "    'romance': top_digrams(romance),\n",
    "    'crime': top_digrams(crime),\n",
    "    'western': top_digrams(western),\n",
    "    'history': top_digrams(history),\n",
    "    'thriller': top_digrams(thriller),\n",
    "    'scifi': top_digrams(scifi)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>comedy</th>\n",
       "      <th>crime</th>\n",
       "      <th>drama</th>\n",
       "      <th>family</th>\n",
       "      <th>history</th>\n",
       "      <th>horror</th>\n",
       "      <th>romance</th>\n",
       "      <th>scifi</th>\n",
       "      <th>thriller</th>\n",
       "      <th>western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year old</td>\n",
       "      <td>best friend</td>\n",
       "      <td>police force</td>\n",
       "      <td>year old</td>\n",
       "      <td>year old</td>\n",
       "      <td>year old</td>\n",
       "      <td>serial killer</td>\n",
       "      <td>high school</td>\n",
       "      <td>life form</td>\n",
       "      <td>tennis player</td>\n",
       "      <td>white man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drug dealers</td>\n",
       "      <td>high school</td>\n",
       "      <td>best friend</td>\n",
       "      <td>nursing home</td>\n",
       "      <td>young boy</td>\n",
       "      <td>old actor</td>\n",
       "      <td>year old</td>\n",
       "      <td>love life</td>\n",
       "      <td>human race</td>\n",
       "      <td>evil organization</td>\n",
       "      <td>cattle thieves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drug dealer</td>\n",
       "      <td>birthday party</td>\n",
       "      <td>family business</td>\n",
       "      <td>members 2nd</td>\n",
       "      <td>night watchman</td>\n",
       "      <td>relationship career</td>\n",
       "      <td>urban legend</td>\n",
       "      <td>year old</td>\n",
       "      <td>year old</td>\n",
       "      <td>Bond mission</td>\n",
       "      <td>young gun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aliens survivors</td>\n",
       "      <td>Their quest</td>\n",
       "      <td>maximum security</td>\n",
       "      <td>state police</td>\n",
       "      <td>true love</td>\n",
       "      <td>humorous meaningful</td>\n",
       "      <td>gas station</td>\n",
       "      <td>best friend</td>\n",
       "      <td>alien spacecraft</td>\n",
       "      <td>secret service</td>\n",
       "      <td>bounty hunters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rescue mission</td>\n",
       "      <td>love life</td>\n",
       "      <td>crime boss</td>\n",
       "      <td>non whites</td>\n",
       "      <td>epic journey</td>\n",
       "      <td>meaningful sequence</td>\n",
       "      <td>young boy</td>\n",
       "      <td>true love</td>\n",
       "      <td>sci fi</td>\n",
       "      <td>time machine</td>\n",
       "      <td>story town</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             action          comedy             crime         drama  \\\n",
       "0          year old     best friend      police force      year old   \n",
       "1      drug dealers     high school       best friend  nursing home   \n",
       "2       drug dealer  birthday party   family business   members 2nd   \n",
       "3  aliens survivors     Their quest  maximum security  state police   \n",
       "4    rescue mission       love life        crime boss    non whites   \n",
       "\n",
       "           family              history         horror      romance  \\\n",
       "0        year old             year old  serial killer  high school   \n",
       "1       young boy            old actor       year old    love life   \n",
       "2  night watchman  relationship career   urban legend     year old   \n",
       "3       true love  humorous meaningful    gas station  best friend   \n",
       "4    epic journey  meaningful sequence      young boy    true love   \n",
       "\n",
       "              scifi           thriller         western  \n",
       "0         life form      tennis player       white man  \n",
       "1        human race  evil organization  cattle thieves  \n",
       "2          year old       Bond mission       young gun  \n",
       "3  alien spacecraft     secret service  bounty hunters  \n",
       "4            sci fi       time machine      story town  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(top_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use plot, rating, and year for all of our classifiers\n",
    "\n",
    "# Initializing vectorizer to use for fitting all of genre plot data\n",
    "vectorizer = TfidfVectorizer(lowercase=False, # Do not convert all to lowercase before tokenizing\n",
    "                          stop_words=None, # Do not remove any stopwords\n",
    "                          ngram_range=(1, 1), # Looking at each word individually ngram = 1\n",
    "                            analyzer=u'word'\n",
    "                          max_df=.5, # Drop words that occur in more than half of the plots\n",
    "                          min_df=1, # Use words that appear at least once (all words)\n",
    "                                \n",
    "                            )\n",
    "\n",
    "# Fit and transform vectorizer for every genere's list of plots\n",
    "# Get all words in the plot data and store terms\n",
    "comPlot = vectorizer.fit_transform(comedy.Plot)\n",
    "com_terms = vectorizer.get_feature_names()\n",
    "horPlot = vectorizer.fit_transform(horror.Plot)\n",
    "hor_terms = vectorizer.get_feature_names()\n",
    "famPlot = vectorizer.fit_transform(family.Plot)\n",
    "fam_terms = vectorizer.get_feature_names()\n",
    "actPlot = vectorizer.fit_transform(action.Plot)\n",
    "act_terms = vectorizer.get_feature_names()\n",
    "criPlot = vectorizer.fit_transform(crime.Plot)\n",
    "cri_terms = vectorizer.get_feature_names()\n",
    "draPlot = vectorizer.fit_transform(drama.Plot)\n",
    "dra_terms = vectorizer.get_feature_names()\n",
    "hisPlot = vectorizer.fit_transform(history.Plot)\n",
    "his_terms = vectorizer.get_feature_names()\n",
    "romPlot = vectorizer.fit_transform(romance.Plot)\n",
    "rom_terms = vectorizer.get_feature_names()\n",
    "sfPlot = vectorizer.fit_transform(scifi.Plot)\n",
    "sf_terms = vectorizer.get_feature_names()\n",
    "thrPlot = vectorizer.fit_transform(thriller.Plot)\n",
    "thr_terms = vectorizer.get_feature_names()\n",
    "wesPlot = vectorizer.fit_transform(western.Plot)\n",
    "wes_terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comedy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for drama is 0.8387096774193549.\n",
      "The score for crime is 0.86.\n"
     ]
    }
   ],
   "source": [
    "# Create data for whether the movie falls under that genre (even as subgenre)\n",
    "# Input is genre specific data (data) and the genre we want to know if it applied (genre)\n",
    "def IsGenre(data,genre):\n",
    "    \n",
    "    # Initialize new list\n",
    "    isGenre = []\n",
    "    \n",
    "    # Iterate over every movie row\n",
    "    for item in data.Genre:\n",
    "        item = item.replace(',','')\n",
    "        # Split genre data into a list\n",
    "        check = item.split(' ')\n",
    "        # Check to see if the genre we are looking for is in the list, append 1 if it is, 0 if it isn't\n",
    "        if genre in check:\n",
    "            isGenre.append(1)\n",
    "        else:\n",
    "            isGenre.append(0)\n",
    "            \n",
    "    # Return binary list of whether movie falls into that genre\n",
    "    return isGenre\n",
    "\n",
    "# Process the plot data and create new dataframe with the processed data\n",
    "# Input is fit/transformed vectorizer (vect), the scraped dataset (data), and the genre we are looking at (genre)\n",
    "def create_data(vect, data, genre):\n",
    "    \n",
    "    # Sum up the vectorizer data into one feature for each movie\n",
    "    tfidf = pd.DataFrame(vect.todense()).sum(axis=1)\n",
    "    \n",
    "    # Create a dataframe with the isGenre data from above function\n",
    "    isGenre = pd.DataFrame(IsGenre(data, genre))\n",
    "    \n",
    "    # Combine all created data to final dataframe\n",
    "    new = pd.concat([tfidf, isGenre, data.loc[:,['Rating','Year']]], axis=1)\n",
    "    # Set column names\n",
    "    new.columns = ['tfidf','isGenre','Rating','Year']\n",
    "    \n",
    "    #Return combined dataframe\n",
    "    return new\n",
    "\n",
    "# Creating all of the data sets for comedy training\n",
    "# Input is genre we are trying to determine whether the movies fall into (gen)\n",
    "def create_training(gen):\n",
    "    comedy_pro = create_data(comPlot, comedy, gen)\n",
    "    horror_pro = create_data(horPlot, horror, gen)\n",
    "    family_pro = create_data(famPlot, family, gen)\n",
    "    action_pro = create_data(actPlot, action, gen)\n",
    "    crime_pro = create_data(criPlot, crime, gen)\n",
    "    drama_pro = create_data(draPlot, drama, gen)\n",
    "    history_pro = create_data(hisPlot, history, gen)\n",
    "    romance_pro = create_data(romPlot, romance, gen)\n",
    "    scifi_pro = create_data(sfPlot, scifi, gen)\n",
    "    thriller_pro = create_data(thrPlot, thriller, gen)\n",
    "    western_pro = create_data(wesPlot, western, gen)\n",
    "    \n",
    "    # Return list of all of the datasets that are in their processed forms for the particular genre\n",
    "    return [comedy_pro, horror_pro, family_pro, action_pro,crime_pro,drama_pro,history_pro,romance_pro,\\\n",
    "           scifi_pro,thriller_pro,western_pro]\n",
    "\n",
    "# Creating the master dataset to use for training\n",
    "com_data = create_training('Comedy')\n",
    "\n",
    "# Creating the training set of data, using first four data sets processed for comedy\n",
    "x_com = pd.concat(com_data[:4])\n",
    "\n",
    "# Initializing the Classification model\n",
    "rfc_com = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting all classifiers with the training data set\n",
    "# Input list of classifiers (lst) and training data (data)\n",
    "def fit_data(lst, data):\n",
    "    for item in lst:\n",
    "        # Binary output\n",
    "        y_train = data.isGenre\n",
    "        \n",
    "        # Drop output data and leave the rest to be trained on\n",
    "        x_train = data.drop(columns=['isGenre'])\n",
    "        \n",
    "        # Fit model to the x and y train data\n",
    "        item.fit(x_train, y_train)\n",
    "\n",
    "# Running fitting function on comedy classifier\n",
    "fit_data([rfc_com], x_com)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "# Input fit classifier (rfc_10), index of the dataset we want to test on (ind), \n",
    "# master dataset for this genre - includes a list of every genre's processed movie data (data)\n",
    "def test_data(rfc_10, ind, data):\n",
    "    # Reference list to pull the name of the genre we are testing\n",
    "    names = ['comedy','horror','family','action','crime','drama',\\\n",
    "             'history','romance','scifi','thriller','western']\n",
    "    # Getting the name of the genre we are testing\n",
    "    gen = names[ind]\n",
    "    \n",
    "    # Getting the dataset for the test data\n",
    "    # Separating out the outcome variable from the data\n",
    "    x_test = data[ind].drop(columns=['isGenre'])\n",
    "    y_test = data[ind].isGenre\n",
    "    \n",
    "    # Printing the accuracy of the test and which genre's data you are testing\n",
    "    print('The score for {} is {}.'.format(gen,rfc_10.score(x_test,y_test)))\n",
    "    return rfc_10.score(x_test,y_test)\n",
    "\n",
    "# Drama\n",
    "comdrama = test_data(rfc_com, 5, com_data)\n",
    "\n",
    "# Crime\n",
    "comcrime = test_data(rfc_com, 4, com_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "comedy_score = np.average([comdrama,comcrime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horror Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for crime is 0.92.\n",
      "The score for drama is 0.967741935483871.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "hor_data = create_training('Horror')\n",
    "x_hor = pd.concat(hor_data[:4])\n",
    "\n",
    "# Initializing horror random forest classifier\n",
    "rfc_hor = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_hor], x_hor)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "horcrime = test_data(rfc_hor, 4, hor_data)\n",
    "hordrama = test_data(rfc_hor, 5, hor_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "horror_score = np.average([hordrama,horcrime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for drama is 1.0.\n",
      "The score for scifi is 0.98.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "fam_data = create_training('Family')\n",
    "x_fam = pd.concat(fam_data[0:5])\n",
    "\n",
    "# Initializing family random forest classifier\n",
    "rfc_fam = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_fam], x_fam)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "famdrama = test_data(rfc_fam, 5, fam_data)\n",
    "famsci = test_data(rfc_fam, 8, fam_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "family_score = np.average([famdrama,famsci])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.96.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "act_data = create_training('Action')\n",
    "x_act = pd.concat(act_data[1:])\n",
    "\n",
    "# Initializing action random forest classifier\n",
    "rfc_act = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_act], x_act)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "actcom = test_data(rfc_act, 0, act_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "action_score = actcom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.72.\n",
      "The score for scifi is 0.82.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "cri_data = create_training('Crime')\n",
    "x_cri = pd.concat(cri_data[2:6])\n",
    "\n",
    "# Initializing crime random forest classifier\n",
    "rfc_cri = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_cri], x_cri)\n",
    "\n",
    "#Testing on a couple genres that were not in training\n",
    "cricom = test_data(rfc_cri, 0, cri_data)\n",
    "crisci = test_data(rfc_cri, 8, cri_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "crime_score = np.average([cricom,crisci])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drama Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.62.\n",
      "The score for horror is 0.5882352941176471.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "dra_data = create_training('Drama')\n",
    "x_dra = pd.concat(dra_data[2:])\n",
    "\n",
    "# Initializing drama random forest classifier\n",
    "rfc_dra = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_dra], x_dra)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "dracom = test_data(rfc_dra, 0, dra_data)\n",
    "drahor = test_data(rfc_dra, 1, dra_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "drama_score = np.average([dracom,drahor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for romance is 0.98.\n",
      "The score for scifi is 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "his_data = create_training('History')\n",
    "x_his = pd.concat(his_data[0:7])\n",
    "\n",
    "# Initializing history random forest classifier\n",
    "rfc_his = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_his], x_his)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "hisrom = test_data(rfc_his, 7, his_data)\n",
    "hissci = test_data(rfc_his, 8, his_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "history_score = np.average([hisrom,hissci])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Romance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.8.\n",
      "The score for scifi is 0.92.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "rom_data = create_training('Romance')\n",
    "x_rom = pd.concat(cri_data[2:8])\n",
    "\n",
    "# Initializing romance random forest classifier\n",
    "rfc_rom = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_rom], x_rom)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "romcom = test_data(rfc_rom, 0, rom_data)\n",
    "romsci = test_data(rfc_rom, 8, rom_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "romance_score = np.average([romcom,romsci])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sciene Fiction Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 0.92.\n",
      "The score for horror is 0.9019607843137255.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "sf_data = create_training('Sci-Fi')\n",
    "x_sf = pd.concat(sf_data[2:9])\n",
    "\n",
    "# Initializing science fiction random forest classifier\n",
    "rfc_sf = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_sf], x_sf)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "scicom = test_data(rfc_sf, 0, sf_data)\n",
    "scihor = test_data(rfc_sf, 1, sf_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "scifi_score = np.average([scicom,scihor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thriller Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 1.0.\n",
      "The score for family is 1.0.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "thr_data = create_training('Thriller')\n",
    "x_thr = pd.concat(thr_data[5:])\n",
    "\n",
    "# Initializing thriller random forest classifier\n",
    "rfc_thr = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_thr], x_thr)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "thrillcom = test_data(rfc_thr, 0, thr_data)\n",
    "thrillfam = test_data(rfc_thr, 2, thr_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "thriller_score = np.average([thrillcom,thrillfam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Western Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for comedy is 1.0.\n",
      "The score for horror is 0.9607843137254902.\n"
     ]
    }
   ],
   "source": [
    "# Creating the master dataset to use for training\n",
    "wes_data = create_training('Western')\n",
    "x_wes = pd.concat(wes_data[5:])\n",
    "\n",
    "# Initializing western random forest classifier\n",
    "rfc_wes = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# Fitting Data\n",
    "fit_data([rfc_wes], x_wes)\n",
    "\n",
    "# Testing on a couple genres that were not in training\n",
    "westcom = test_data(rfc_wes, 0, wes_data)\n",
    "westhor = test_data(rfc_wes, 1, wes_data)\n",
    "\n",
    "# Getting score for main genre data\n",
    "western_score = np.average([westcom,westhor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting average scores for classifiers when tested on datasets they were not trained on\n",
    "# Labels for plot\n",
    "names = ['comedy','horror','family','action','crime','drama',\\\n",
    "             'history','romance','scifi','thriller','western']\n",
    "\n",
    "# Average scores for plots\n",
    "genre_scores = [comedy_score,horror_score, family_score, action_score,crime_score,drama_score, \n",
    "                history_score,romance_score,scifi_score, thriller_score,western_score]\n",
    "\n",
    "\n",
    "# Plotting data\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,5) #Made it longer in length to fit presentation\n",
    "gs = sns.barplot(x=names, y=genre_scores, ax=ax)\n",
    "gs.set_title('Comparison of Genre Scores')\n",
    "gs.set_ylabel('Scores')\n",
    "gs.set_xlabel('Genres')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the classifiers perform well, better than expected with the limited amount of training data fed into them. It is possible that these models overfit, but it can be seen through testing them on genre groups that were not contained in teh training set - they still perform very well. The lowest performing classifier is Drama, which is a bit unavoidable considering it's overlap with many different genres including Comedy, Romance, and Thriller. With that being said, we are still predicting above 70% on the testing sets so we can conclude that this classifier also performs well and move on to the prediction portion of our program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting most topical sentence from input plot paragraph\n",
    "def get_summary(data_plot_sent):\n",
    "    #Load parser in english\n",
    "    parser = spacy.load('en')\n",
    "\n",
    "    # Parsing the input plot data\n",
    "    plot_doc = parser(data_plot_sent)\n",
    "\n",
    "    # Dividing the text into sentences and storing them as a list of strings.\n",
    "    sentences=[]\n",
    "    for span in plot_doc.sents:\n",
    "        # Go from the start to the end of each span, returning each token in the sentence\n",
    "        # Combine each token using join()\n",
    "        sent = ''.join(plot_doc[i].string for i in range(span.start, span.end)).strip()\n",
    "        # Add each sentence to list of sentences\n",
    "        sentences.append(sent)\n",
    "    \n",
    "    # Initialize vectorizer for plot summary\n",
    "    vect_word = TfidfVectorizer(lowercase=False, # Don't change to lowercase\n",
    "                              stop_words='english', # Load stop words in english\n",
    "                              ngram_range=(1, 1),  # Look at each word individually\n",
    "                              analyzer=u'word', # N-gram per word\n",
    "                              min_df=1, # Use all words (appear at least once)\n",
    "                                )\n",
    "    \n",
    "    # Fit and Transform vectorizer with sentences list\n",
    "    tfidf = vect_word.fit_transform(sentences)\n",
    "    \n",
    "    # Calculating similarity\n",
    "    similarity = tfidf * tfidf.T\n",
    "\n",
    "    # Identifying the sentence with the highest rank.\n",
    "    nx_graph = nx.from_scipy_sparse_matrix(similarity)\n",
    "    ranks=nx.pagerank(nx_graph, tol=.00000001)\n",
    "\n",
    "    ranked = sorted(((ranks[i],s) for i,s in enumerate(sentences)),\n",
    "                    reverse=True)\n",
    "\n",
    "    # Return sentence that is most similar (most topical sentence)\n",
    "    return ranked[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test data from the title inputs\n",
    "def create_fav_data(fav_info):\n",
    "    # Initialize vectorizer for new input data\n",
    "    vect = TfidfVectorizer( \n",
    "                          stop_words=None, # Use all the data\n",
    "                          ngram_range=(1, 1), #n=1, look at words individually\n",
    "                          min_df=1 # All words\n",
    "                          )\n",
    "    # Fit and Transform vectorizer with plot of input data\n",
    "    fav_plot = vect.fit_transform(fav_info.Plot)\n",
    "    # Create test data using previously made function with input data\n",
    "    info = create_data(fav_plot,fav_info, 'Comedy')\n",
    "    \n",
    "    # Return test data without the isGenre column\n",
    "    return info.drop(columns=['isGenre'])\n",
    "\n",
    "# Scraper to get movie data from IMDb Api (OMDb) from input titles \n",
    "def get_movie_info(titles):\n",
    "    importlist = titles\n",
    "    \n",
    "    #Define crawler and processer\n",
    "    class ImportSpider(scrapy.Spider):\n",
    "        name = \"ImportedMovie\"\n",
    "        start_urls=[]\n",
    "\n",
    "        # Initiating Start URLs\n",
    "        for i in range(len(importlist)):\n",
    "            item = str(importlist[i])\n",
    "            start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "        # Identifying the information we want from the query response and extracting it using xpath.\n",
    "        def parse(self, response):\n",
    "            for item in response.xpath('//movie'):\n",
    "                # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "                yield {\n",
    "                        'Title': item.xpath('@title').extract_first(),\n",
    "                    'Year': item.xpath('@year').extract_first(),\n",
    "                    'Genre': item.xpath('@genre').extract_first(),\n",
    "                    'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "                    'Plot': item.xpath('@plot').extract_first()\n",
    "\n",
    "                }\n",
    "            \n",
    "    process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': 'ImportedData.json',\n",
    "    # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "    'ROBOTSTXT_OBEY': False,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True,\n",
    "    'LOG_ENABLED': False   \n",
    "    })\n",
    "    \n",
    "    #Delete file if it already exists\n",
    "    if os.path.exists('ImportedData.json'):\n",
    "        os.remove('ImportedData.json')\n",
    "    \n",
    "    #Run crawler\n",
    "    process.crawl(ImportSpider)\n",
    "    process.start()\n",
    "    \n",
    "    #Read in data created\n",
    "    info = pd.read_json('ImportedData.json')\n",
    "    #Create and return the testable data for the scraped data from titles\n",
    "    data = create_fav_data(info)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the first movie you like?When Harry met Sally\n",
      "What is the second movie you like?10 Things I Hate About You\n",
      "What is the third movie you like?Crazy Rich Asians\n",
      "\n",
      " You should watch The Shawshank Redemption. \n",
      " You will like it because: \n",
      " The film portrays the man's unique way of dealing with his new, torturous life; along the way he befriends a number of fellow prisoners, most notably a wise long-term inmate named Red.\n"
     ]
    }
   ],
   "source": [
    "# List of Genres and their corresponding classifiers\n",
    "classifiers = [(\"Comedy\", rfc_com), (\"Horror\", rfc_hor),(\"Family\", rfc_fam),(\"Action\",rfc_act),\\\n",
    "              (\"Crime\", rfc_cri),(\"Drama\",rfc_dra),(\"History\",rfc_his),(\"Romance\",rfc_rom),\\\n",
    "              (\"Science Fiction\",rfc_sf),(\"Thriller\",rfc_thr),(\"Western\",rfc_wes)]\n",
    "\n",
    "# List of Genres and their corresponding list of titles\n",
    "titles = [(\"Comedy\", comedy_title), (\"Horror\", horror_title),(\"Family\", family_title),(\"Action\",action_title),\\\n",
    "              (\"Crime\", crime_title),(\"Drama\",drama_title),(\"History\",history_title),(\"Romance\",romance_title),\\\n",
    "              (\"Science Fiction\",scifi_title),(\"Thriller\",thriller_title),(\"Western\",western_title)]\n",
    "\n",
    "# List of Genres and their corresponding original scraped datasets\n",
    "datas = [(\"Comedy\", comedy), (\"Horror\", horror),(\"Family\", family),(\"Action\",action),\\\n",
    "              (\"Crime\", crime),(\"Drama\",drama),(\"History\",history),(\"Romance\",romance),\\\n",
    "              (\"Science Fiction\",scifi),(\"Thriller\",thriller),(\"Western\",western)]\n",
    "\n",
    "# Getting profile (list of genres) for the three inputs\n",
    "# Input is list of classifiers, and the input movie data (test_input)\n",
    "def get_profile(cfrs, test_input):\n",
    "    \n",
    "    #Initialize empty list and max input\n",
    "    profile = []\n",
    "    max_input=0\n",
    "    \n",
    "    # Get the highest occurance of movies in a particular genre\n",
    "    for item in cfrs:\n",
    "        # For each item separate genre name and classifier\n",
    "        genre = item[0]\n",
    "        cf = item[1]\n",
    "        \n",
    "        # For each genre, predict whether the movie input belongs in that genre\n",
    "        prediction = cf.predict(test_input)\n",
    "        \n",
    "        # Sum the predictions to see how many movies fit that genre\n",
    "        if prediction.sum() > max_input:\n",
    "            # Update max_input \n",
    "            max_input = prediction.sum()\n",
    "    \n",
    "    # Use highest occurance of movies to find the most popular genres\n",
    "    for item in cfrs:\n",
    "        # For each item separate genre name and classifier\n",
    "        genre = item[0]\n",
    "        cf = item[1]\n",
    "        \n",
    "        # For each genre, predict whether the movie input belongs in that genre\n",
    "        prediction = cf.predict(test_input)\n",
    "        \n",
    "        # Only add the genre to the list if it occurs the same amount of times as the max\n",
    "        if prediction.sum() == max_input:\n",
    "            profile.append(genre)\n",
    "    \n",
    "    # Return a list of all of the genres that the inputs fall into\n",
    "    return profile\n",
    "\n",
    "# Program to get recommendations\n",
    "def get_recommendations(cfrs, ttl):\n",
    "    \n",
    "    # Initialize Movie List\n",
    "    movie_list = []\n",
    "    \n",
    "    # Add the titles of the movies that the user inputs\n",
    "    movie_list.append(input(\"What is the first movie you like?\"))\n",
    "    movie_list.append(input(\"What is the second movie you like?\"))\n",
    "    movie_list.append(input(\"What is the third movie you like?\"))\n",
    "    \n",
    "    # Transform the input movie list to movie data\n",
    "    actual_input = get_movie_info(movie_list)\n",
    "    fav_info = actual_input\n",
    "    \n",
    "    # Get list of most applicable genres\n",
    "    profile = get_profile(cfrs, fav_info)\n",
    "    \n",
    "    # Random pick from liked genres\n",
    "    ind = random.randint(0,len(profile)-1)\n",
    "    genre = profile[ind]\n",
    "    \n",
    "    # Getting title list\n",
    "    for item in ttl:\n",
    "        if item[0] == genre:\n",
    "            ttls = item[1]\n",
    "            break\n",
    "    \n",
    "    # Random pick in appropriate genre\n",
    "    ind2 = random.randint(0,len(ttls)-1)\n",
    "    pick = ttls.iloc[ind2,0]\n",
    "    \n",
    "    # Getting plot data from the selected movie for summarization\n",
    "    for g in datas:\n",
    "        if g[0] == genre:\n",
    "            plot_data = g[1]\n",
    "    \n",
    "    # Running summary function on plot for selected movie\n",
    "    plot_sum = get_summary(plot_data.iloc[ind2,:].Plot)\n",
    "\n",
    "    return pick, plot_sum\n",
    "\n",
    "pick, plot_summ = get_recommendations(classifiers,titles)\n",
    "\n",
    "print(\"\\n You should watch {}. \\n You will like it because: \\n {}\".format(pick, plot_summ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our program is a bit limited in that it can only suggest a movie from the initial title list scraped from Ranker - it would be greatly improved if it had an expansive list of all popular movies in a particular genre to train to both train the classifiers on and suggest from. But we did what we set out to do, which was to recommend a movie based on movies that the user likes. A key element of this program is that any valid movie can be input because we scrape the data from IMDB, so we do not limit ourselves on the movies that can be input.\n",
    "\n",
    "This program does need access to the internet to scrape the data for the input movies, but other than that it is pretty self contained and could work well in a production environment. The need of running a scraper as part of the program is a bit of a limitation as the Kernel would need to be killed in a program to restart it if there is a similar task that needs to be completed. To be completely self-sufficient, this program would require the storage of between 1000-10000 rows of movie data, which is reasonable in terms of storage space.\n",
    "\n",
    "Overall, I would say we successfully recommended a movie based on the current system storage limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix:\n",
    "### A. Gathering information.\n",
    "Collecting popular titles by genre from Ranker.com via webscraper and collecting the movie data (plot, ranking, and year) from IMDB using their open api OMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class ComedySpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Comedy\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/100-all-time-greatest-comedy-films/',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Comedy.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(ComedySpider)\n",
    "# process.start()\n",
    "# print('Comedy Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class HorrorSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Horror\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/the-greatest-horror-films-ever-made/',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Horror.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(HorrorSpider)\n",
    "# process.start()\n",
    "# print('Horror Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class ActionSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Action\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/best-action-movies',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Action.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(ActionSpider)\n",
    "# process.start()\n",
    "# print('Action Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class CrimeSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Crime\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/all-crime-movies-or-list-of-crime-movies/all-genre-movies-lists',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Crime.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(CrimeSpider)\n",
    "# process.start()\n",
    "# print('Crime Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class DramaSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Drama\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-intelligent-dramas/ranker-film?ref=search',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Drama.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(DramaSpider)\n",
    "# process.start()\n",
    "# print('Drama Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class FamilySpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Family\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-pg-family-film-movies/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Family.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(FamilySpider)\n",
    "# process.start()\n",
    "# print('Family Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class HistorySpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"History\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-pg-13-history-movies/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'History.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(HistorySpider)\n",
    "# process.start()\n",
    "# print('History Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class RomanceSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Romance\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/romance-film-movies-and-films/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Romance.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(RomanceSpider)\n",
    "# process.start()\n",
    "# print('Romance Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class SciFiSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"SciFi\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/classic-science-fiction-movies/ranker-film',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'SciFi.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(SciFiSpider)\n",
    "# process.start()\n",
    "# print('Sci-Fi Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class ThrillerSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Thriller\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/list/best-pg-thriller-movies/reference',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Thriller.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(ThrillerSpider)\n",
    "# process.start()\n",
    "# print('Thriller Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Running scrapers to collect movie titles by genre\n",
    "# # Importing in each cell because of the kernel restarts.\n",
    "# import scrapy\n",
    "# import re\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# class WesternSpider(scrapy.Spider):\n",
    "#     # Naming the spider is important if you are running more than one spider of\n",
    "#     # this class simultaneously.\n",
    "#     name = \"Western\"\n",
    "    \n",
    "#     # URL(s) to start with.\n",
    "#     start_urls = [\n",
    "#         'https://www.ranker.com/crowdranked-list/the-best-western-movies-ever-made',\n",
    "#     ]\n",
    "\n",
    "#     # Use XPath to parse the response we get.\n",
    "#     def parse(self, response):\n",
    "        \n",
    "#         # Iterate over every <article> element on the page.\n",
    "#         for article in response.xpath('//h2'):\n",
    "            \n",
    "#             # Yield a dictionary with the values we want.\n",
    "#             yield {\n",
    "#                 'name': article.xpath('div[@class=\"listItem__data\"]/a/text()').extract_first()\n",
    "#             }\n",
    "\n",
    "# # Tell the script how to run the crawler by passing in settings.\n",
    "# # The new settings have to do with scraping etiquette.          \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "#     'FEED_URI': 'Western.json',       # Name our storage file.\n",
    "#     'LOG_ENABLED': False,          # Turn off logging for now.\n",
    "#     'ROBOTSTXT_OBEY': True,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True\n",
    "    \n",
    "# })\n",
    "\n",
    "# # Start the crawler with our spider.\n",
    "# process.crawl(WesternSpider)\n",
    "# process.start()\n",
    "# print('Western Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class CMSpider(scrapy.Spider):\n",
    "#     name = \"Movie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(comedy_title)):\n",
    "#         item = str(comedy_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'CMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False,\n",
    "#     # We use CLOSESPIDER_PAGECOUNT to limit our scraper to the first 100 links.    \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(CMSpider)\n",
    "# process.start()\n",
    "# print('Comedy Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class HorMSpider(scrapy.Spider):\n",
    "#     name = \"HorrorMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(horror_title)):\n",
    "#         item = str(horror_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'HorMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(HorMSpider)\n",
    "# process.start()\n",
    "# print('Horror Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class ActMSpider(scrapy.Spider):\n",
    "#     name = \"ActionMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(action_title)):\n",
    "#         item = str(action_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'ActMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False  \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(ActMSpider)\n",
    "# process.start()\n",
    "# print('Action Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class CriMSpider(scrapy.Spider):\n",
    "#     name = \"CrimeMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(crime_title)):\n",
    "#         item = str(crime_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'CriMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(CriMSpider)\n",
    "# process.start()\n",
    "# print('Crime Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class DraMSpider(scrapy.Spider):\n",
    "#     name = \"DramaMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(drama_title)):\n",
    "#         item = str(drama_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'DraMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False    \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(DraMSpider)\n",
    "# process.start()\n",
    "# print('Drama Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class FamMSpider(scrapy.Spider):\n",
    "#     name = \"FamilyMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(family_title)):\n",
    "#         item = str(family_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'FamMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(FamMSpider)\n",
    "# process.start()\n",
    "# print('Family Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class HisMSpider(scrapy.Spider):\n",
    "#     name = \"HistoryMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(history_title)):\n",
    "#         item = str(history_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'HisMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(HisMSpider)\n",
    "# process.start()\n",
    "# print('History Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class RomMSpider(scrapy.Spider):\n",
    "#     name = \"RomanceMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(romance_title)):\n",
    "#         item = str(romance_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'RomMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False    \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(RomMSpider)\n",
    "# process.start()\n",
    "# print('Romance Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class SFMSpider(scrapy.Spider):\n",
    "#     name = \"SciFiMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(scifi_title)):\n",
    "#         item = str(scifi_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'SFMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False   \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(SFMSpider)\n",
    "# process.start()\n",
    "# print('Sci-Fi Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class ThrMSpider(scrapy.Spider):\n",
    "#     name = \"ThrillerMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(thriller_title)):\n",
    "#         item = str(thriller_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'ThrMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False\n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(ThrMSpider)\n",
    "# process.start()\n",
    "# print('Thriller Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class WesMSpider(scrapy.Spider):\n",
    "#     name = \"WesternMovie\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(western_title)):\n",
    "#         item = str(western_title.name.loc[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'WesMovies.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False, \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(WesMSpider)\n",
    "# process.start()\n",
    "# print('Western Data Extracted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scraping Comedy Data\n",
    "# import scrapy\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "# class InputSpider(scrapy.Spider):\n",
    "#     name = \"MovieInput\"\n",
    "#     start_urls=[]\n",
    "    \n",
    "#     input_titles = ['Batman', 'Lego Movie','Avengers']\n",
    "    \n",
    "#     # Initiating Start URLs\n",
    "#     for i in range(len(input_titles)):\n",
    "#         item = str(input_titles[i])\n",
    "#         start_urls.append('http://www.omdbapi.com/?apikey=236b0bfb&r=xml&plot=full&t={}'.format(item))\n",
    "\n",
    "#     # Identifying the information we want from the query response and extracting it using xpath.\n",
    "#     def parse(self, response):\n",
    "#         for item in response.xpath('//movie'):\n",
    "#             # The ns code identifies the type of page the link comes from.  '0' means it is a Wikipedia entry.\n",
    "#             yield {\n",
    "#                     'Title': item.xpath('@title').extract_first(),\n",
    "#                 'Year': item.xpath('@year').extract_first(),\n",
    "#                 'Genre': item.xpath('@genre').extract_first(),\n",
    "#                 'Rating': item.xpath('@imdbRating').extract_first(),\n",
    "#                 'Plot': item.xpath('@plot').extract_first()\n",
    "                \n",
    "#                     }\n",
    "                 \n",
    "    \n",
    "# process = CrawlerProcess({\n",
    "#     'FEED_FORMAT': 'json',\n",
    "#     'FEED_URI': 'MovieInput.json',\n",
    "#     # Note that because we are doing API queries, the robots.txt file doesn't apply to us.\n",
    "#     'ROBOTSTXT_OBEY': False,\n",
    "#     'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "#     'AUTOTHROTTLE_ENABLED': True,\n",
    "#     'HTTPCACHE_ENABLED': True,\n",
    "#     'LOG_ENABLED': False, \n",
    "# })\n",
    "                                         \n",
    "\n",
    "# # Starting the crawler with our spider.\n",
    "# process.crawl(InputSpider)\n",
    "# process.start()\n",
    "# print('Movie Input Extracted!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
